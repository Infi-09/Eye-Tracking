{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dde96297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0500b",
   "metadata": {},
   "source": [
    "Let's create our EyeTracker class with the track method. First, define the __init__ method and instantiate the faceCascade and Eyecascade using **CascadeClassifier** in opencv and passing the xml file for two cascade. Then, define the track method which takes the image as input and outputs the tracked rectangle points list. In the **track** method,, first detect the face using **facecascade** and detectMultiScale function and returns the rect of face and then pass the ROI to eyecascade to detect the eyes in the faces using **eyecacade** and **detectMultiScale** which returns the rect of the eyes in the face which have been detected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbec963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeTracker:\n",
    "    def __init__(self, faceCascadePath, eyeCascadePath):\n",
    "        self.faceCascade = cv2.CascadeClassifier(faceCascadePath)\n",
    "        self.eyeCascade = cv2.CascadeClassifier(eyeCascadePath)\n",
    "        \n",
    "    def track(self, image):\n",
    "        rects = []\n",
    "        faceRects = self.faceCascade.detectMultiScale(image, \n",
    "                                                     scaleFactor=1.1,\n",
    "                                                     minNeighbors=5,\n",
    "                                                     minSize=(30, 30),\n",
    "                                                     flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        for (fX, fY, fW, fH) in faceRects:\n",
    "            faceROI = image[fY:fY + fH, fX:fX + fW] \n",
    "            rects.append((fX, fY, fX+fW, fH+fY))\n",
    "            \n",
    "            eyeRects = self.eyeCascade.detectMultiScale(faceROI, \n",
    "                                                    scaleFactor=1.1,\n",
    "                                                    minNeighbors=10,\n",
    "                                                    minSize=(20, 20),\n",
    "                                                    flags=cv2.CASCADE_SCALE_IMAGE)    \n",
    "        \n",
    "            for (eX, eY, eW, eH) in eyeRects:\n",
    "                rects.append((fX+eX, fY+eY, fX+eX+eW, fY+eY+eH))\n",
    "            \n",
    "        return rects    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8003d",
   "metadata": {},
   "source": [
    "### Parameters used in the detectMultiScale function \n",
    "**scaleFactor**: How much the image size is reduced at each image scale. This value is used to create the scale pyramid in order to detect faces at multiple scales in the image (some faces may be closer to the foreground, and thus be larger; other faces may be smaller and in the background, thus the usage of varying scales). A value of 1.05 indicates that Jeremy is reducing the size of the image by 5% at each level in the pyramid.                                                                                      \n",
    "\n",
    "**minNeighbors**: How many neighbors each windo w should have for the area in the window to be considered a face. The cascade classifier will detect multiple windows around a face. This parameter controls how many rectangles (neighbors) need to be detected for the window to be labeled a face.\n",
    "\n",
    "**minSize**: A tuple of width and height (in pixels) indicating the minimum size of the window. Bounding boxes smaller than this size are ignored. It is a good idea to start with (30, 30) and fine-tune from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88f3f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize the video for our convenience\n",
    "def resize(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    (h, w) = image.shape[:2]\n",
    "    dim = None\n",
    "    \n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    \n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(r * w), height)\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(r * h))\n",
    "        \n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "354e3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object our our EyeTracker class and arguments as two xml files (face, eye)\n",
    "et = EyeTracker(\"cascade/haarcascade_frontalface_default.xml\",\n",
    "                \"cascade/haarcascade_eye.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4194c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the video method (Here webcam)\n",
    "video = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95b14ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to iter the frames\n",
    "while True:\n",
    "    (grabbed, frame) = video.read()\n",
    "    \n",
    "    # Only work if grabbed is True\n",
    "    if not grabbed:\n",
    "        break\n",
    "    \n",
    "    # Resize the video   \n",
    "    frame = resize(frame, width=1000)    \n",
    "    # Change to gray scale frame\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # rects - has the rectangle points for face and eyes\n",
    "    rects = et.track(gray)\n",
    "    \n",
    "    # Loop the rects \n",
    "    for rect in rects:\n",
    "        cv2.rectangle(frame, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 0), 2)\n",
    "    \n",
    "    # Dispaly the frame\n",
    "    cv2.imshow(\"Eye Tracking\", frame)    \n",
    "    \n",
    "    # Close the webcam if, pressed \"q\"\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "# Stop the video\n",
    "video.release()\n",
    "# Destroy all the windows ehich has been opened by OpenCV\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a09b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffceb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
